{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import random as ran\n",
    "import math as math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_clusters_conditional_probability(clusters_number):\n",
    "    initial_cond_prob = np.zeros((Npts,clusters_number))\n",
    "    for i in range(Npts):\n",
    "        x = np.random.rand(clusters_number)\n",
    "        initial_cond_prob[i] = x/np.sum(x)\n",
    "    return initial_cond_prob.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mine = generate_random_clusters_conditional_probability(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 200, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_number =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_clusters_conditional_probability(clusters_number):\n",
    "    initial_cond_prob = np.zeros((Npts, clusters_number))\n",
    "    for i in range(Npts):\n",
    "        x = np.random.rand(clusters_number)\n",
    "        initial_cond_prob[i] = x/np.sum(x)\n",
    "    return initial_cond_prob.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Npts, Ndim = 200, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_initial_cond_prob = generate_random_clusters_conditional_probability(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_initial_cond_prob[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_initial_cond_prob[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_points' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-d036ab479b05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistance_matrix_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdistance_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_points\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_points\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# define similarities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_points' is not defined"
     ]
    }
   ],
   "source": [
    "distance_matrix_= distance_matrix(data_points,data_points) # define similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define core function of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test_initial_cond_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors_matrix = np.full((Npts, 1), 1/Npts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48819907, 0.51180093]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(test_initial_cond_prob, priors_matrix).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48942318, 0.51057682])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(test_initial_cond_prob, priors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##yours - A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_convert_data_points(filename):\n",
    "    splits = []\n",
    "    with open(filename, 'r') as f:\n",
    "        data_points = f.readlines()\n",
    "    \n",
    "    for data_point in data_points:\n",
    "        splits.append(data_point.split())\n",
    "    \n",
    "    return np.array(splits, dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blahut_algorithm_imp(clusters_number,\n",
    "                              priors,\n",
    "                              distance_matrix,\n",
    "                              beta, \n",
    "                              rounds=20,\n",
    "                              threshold=1e-10\n",
    "                             ):\n",
    "\n",
    "    priors_matrix = np.full((Npts, 1), 1/float(Npts))\n",
    "    D_results=[]\n",
    "     \n",
    "    for round in range(rounds):\n",
    "        D = []\n",
    "        lagrange_multiplier=[]\n",
    "        diff_elem_value=1\n",
    "        conditional_prob = initial_clustering_membership_probability(clusters_number)\n",
    "        evidence_vector = np.zeros((clusters_number,))\n",
    "        posteriori_prob = np.zeros((Npts, clusters_number)).transpose()\n",
    "        element_cluster_distance_matrix = np.zeros((Npts, clusters_number))\n",
    "        normalization_partition_funct = np.zeros((Npts, ))\n",
    "\n",
    "        while (diff_elem_value > threshold):\n",
    "  \n",
    "            for c in range(clusters_number):\n",
    "\n",
    "            #     Computes the evidence for each cluster 'C': returns a vector   \n",
    "                evidence_vector[c] = np.sum(np.dot(conditional_prob[c], priors))\n",
    "            #     Computes the posterior probability: returns a matrix C x Npts\n",
    "                posteriori_prob[c] = (np.dot(priors, conditional_prob[c]))/evidence_vector[c]    \n",
    "\n",
    "            #     Compute element to cluster distance:\n",
    "                for i in range(Npts):\n",
    "                    element_cluster_distance_matrix[i][c] = sum(\n",
    "                                            [posteriori_prob[c][j_idx]* distance_data_point\n",
    "                                             for j_idx, distance_data_point in enumerate(distance_matrix[i])]\n",
    "                                                    ) \n",
    "            #compute partition function:\n",
    "            exponential_beta_matrix = np.exp(-beta * element_cluster_distance_matrix)\n",
    "            normalization_partition_funct = (np.multiply(evidence_vector, exponential_beta_matrix)).sum(axis=1)\n",
    "\n",
    "            #update conditionnal cluster_proba\n",
    "            conditional_prob = (np.multiply\n",
    "                                (evidence_vector, \n",
    "                                 exponential_beta_matrix)/ normalization_partition_funct[:,None]\n",
    "                               ).transpose()\n",
    "\n",
    "            #Compute averaged element to cluster distance:\n",
    "            joint_probability = np.multiply(evidence_vector, posteriori_prob.transpose())\n",
    "            d =np.sum(np.multiply(joint_probability,element_cluster_distance_matrix))\n",
    "\n",
    "            D.append(d)\n",
    "            ##Compute Compression rate:       \n",
    "            ratios = posteriori_prob.transpose()/priors\n",
    "            compression_rate_ = np.sum(np.multiply(joint_probability, np.log2(ratios)))\n",
    "            #compression_rate.append(compression_rate_)\n",
    "\n",
    "            ##Compute Lagrange Multiplier:\n",
    "            lagrange_multiplier_ = compression_rate_ + beta*d\n",
    "            lagrange_multiplier.append(lagrange_multiplier_)     \n",
    "\n",
    "            if len(lagrange_multiplier) > 1:\n",
    "                diff_elem_value = lagrange_multiplier[-2]-lagrange_multiplier[-1]\n",
    "                #diff_lm.append(diff_elem_value)\n",
    "        D_results.append(D[-1])\n",
    "        mean_D_results = sum(D_results)/float(rounds)\n",
    "    return mean_D_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_clustering_membership_probability(clusters_number):\n",
    "    \"\"\"\n",
    "    generate_random_clusters_conditional_probability is a function that takes as an argument the \n",
    "    number of clusters 'C' and generates a cluster membership probability per element 'i' in the dataset\n",
    "    composed by 'N'elements.\n",
    "    The sum of an element cluster membership probabilities shall be 1.\n",
    "    param clusters_number: 'C'\n",
    "    return initial_cond_prob: C x N matrix. \n",
    "    \"\"\" \n",
    "    initial_cond_prob = np.zeros((Npts,clusters_number))\n",
    "    for i in range(Npts):\n",
    "        x = np.random.rand(clusters_number)\n",
    "        initial_cond_prob[i] = x/np.sum(x)\n",
    "    return initial_cond_prob.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_perform_algorithm(range_betas_, prior_, distance_matrix_, range_clusters=range(2, 5)):\n",
    "    results = np.zeros((len(range_clusters),len(range_betas_)))\n",
    "    betas=np.zeros((len(range_betas_)))   \n",
    "    coordenates = np.zeros((len(range_clusters), len(range_betas_),2))\n",
    "    \n",
    "    for idx, cluster_n in enumerate(range_clusters):\n",
    "        print(cluster_n)\n",
    "        for idx_beta, beta in enumerate(range_betas_):\n",
    "            print(beta)\n",
    "            results[idx, idx_beta] = blahut_algorithm_imp(cluster_n,prior,distance_matrix_,beta)\n",
    "            betas[idx_beta]=beta\n",
    "    \n",
    "    for idx, cluster in enumerate(range_clusters):\n",
    "        coordenates[idx].transpose()[0] = (-results[idx])\n",
    "        coordenates[idx].transpose()[1] = betas\n",
    "\n",
    "    return coordenates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_conditionnal_cluster_proba(conditionnal_cluster_proba,\n",
    "                                      prior_proba,\n",
    "                                      distance_matrix):\n",
    "    \"\"\"\n",
    "    Updates p(x_alpha|xi) according to bottleneck algorithm and the self-consistent equations.\n",
    "    \n",
    "    :param conditionnal_cluster_proba: a (cluster_number, Npts) matrix representing p(x_alpha| xi).\n",
    "    :param prior_proba: a Npts length vector representing the probability prior of the data points.\n",
    "    :param distance_matrix: a (Npts, Npts) shape matrix of the distance between all possible pair of\n",
    "        data points.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # basically this is the 'Algorithm implementation' that you made, and that I tried to cut it more properly\n",
    "    # in its different steps, for better readability :) . A number of things you could copy paste...\n",
    "    # maybe try to respect more or less the structure as it will be easier for you to focus on each step\n",
    "    # separately; i know i get confused easily so when i can i do it\n",
    "    \n",
    "    \n",
    "    # Computes the evidence:\n",
    "    evidence_vector = np.zeros((clusters_number,))\n",
    "    # (n_cluster, ) = (cluster_number, Npts) * (Npts, )  <-> means matrix shapes multiplication compatibility\n",
    "    # ... p(x_alpha) = sum( p(x_alpha| xi) * p(xi), i=1..Npts), as a matrix product formula:\n",
    "    evidence_vector = np.matmul(conditionnal_cluster_proba, priors_matrix)\n",
    "    \n",
    "    # now your turn :) ... \n",
    "    \n",
    "    # Computes the posterior proba\n",
    "    posteriori_proba_matrix = np.zeros((Npts, clusters_number))\n",
    "    # ... either a loop that's gonna fill all posteriori_proba_matrix[data_pt_idx, cluster_idx] correctly\n",
    "    # according to eq 4, or a matrix product if you want to do it more compactly\n",
    "    # (also is optimized but harder to pinpoint)\n",
    "    # don't forget to divide by evidence factor as you did already :)\n",
    "    \n",
    "    # compute element to cluster distance:\n",
    "    element_cluster_distance_matrix = np.zeros((Npts, clusters_number))\n",
    "    # handled also with matrix: d(xi, x_alpha) <-> (Npts, cluster_number) = (Npts, Npts) * (Npts, cluster_number),\n",
    "    # with d(xi, x_alpha) = sum(p(xj|x_alpha) * d(xi, xj), j=1...Npts) , you can write a matrix product.\n",
    "    \n",
    "    # compute partition function:\n",
    "    partition_funct = np.zeros((Npts, ))\n",
    "    # hint: in numpy: a = np.zeros((2,2)),  np.exp(a) = np.array([exp(0), exp(0)], [exp(0), exp(0)])\n",
    "    exponential_beta_matrix = np.exp(-beta * element_cluster_distance_matrix)\n",
    "    # Z(xi, B) <-> (Npts,) = (Npts, n_cluster) * (n_cluster, ), with:\n",
    "    # Z(xi, B) = sum(p(x_alpha)*exp(-Bd(xi, x_alpha), alpha=1..Nc)\n",
    "    \n",
    "    # update conditionnal cluster_proba\n",
    "    new_conditionnal_cluster_proba = evidence * exponential_beta_matrix / partition_funct\n",
    "    \n",
    "    return new_conditionnal_cluster_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = read_and_convert_data_points('Data_Exercise4_2019')\n",
    "prior = 1/float(Npts)\n",
    "Npts, Ndim = data_points.shape\n",
    "distance_matrix_= distance_matrix(data_points,data_points) # define similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = []\n",
    "diff_elem_value=1\n",
    "conditional_prob = initial_clustering_membership_probability(clusters_number)\n",
    "evidence_vector = np.zeros((clusters_number,))\n",
    "posteriori_prob = np.zeros((Npts, clusters_number)).transpose()\n",
    "element_cluster_distance_matrix = np.zeros((Npts, clusters_number))\n",
    "normalization_partition_funct = np.zeros((Npts, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blahut_algorithm_imp(clusters_number,\n",
    "                              priors,\n",
    "                              distance_matrix,\n",
    "                              beta, \n",
    "                              rounds=20,\n",
    "                              threshold=1e-10\n",
    "                             ):\n",
    "\n",
    "    priors_matrix = np.full((Npts, 1), 1/float(Npts))\n",
    "    D_results=[]\n",
    "     \n",
    "    for round in range(rounds): \n",
    "        D = []\n",
    "        diff_elem_value=1\n",
    "        conditional_prob = initial_clustering_membership_probability(clusters_number)\n",
    "        evidence_vector = np.zeros((clusters_number,))\n",
    "        posteriori_prob = np.zeros((Npts, clusters_number)).transpose()\n",
    "        element_cluster_distance_matrix = np.zeros((Npts, clusters_number))\n",
    "        normalization_partition_funct = np.zeros((Npts, ))\n",
    "\n",
    "        while (diff_elem_value > threshold):\n",
    "  \n",
    "            for c in range(clusters_number):\n",
    "\n",
    "            #     Computes the evidence for each cluster 'C': returns a vector   \n",
    "                evidence_vector[c] = np.sum(np.dot(conditional_prob[c], priors))\n",
    "            #     Computes the posterior probability: returns a matrix C x Npts\n",
    "                posteriori_prob[c] = (np.dot(priors, conditional_prob[c]))/evidence_vector[c]    \n",
    "\n",
    "            #     Compute element to cluster distance:\n",
    "                for i in range(Npts):\n",
    "                    element_cluster_distance_matrix[i][c] = sum(\n",
    "                                            [posteriori_prob[c][j_idx]* distance_data_point\n",
    "                                             for j_idx, distance_data_point in enumerate(distance_matrix[i])]\n",
    "                                                    ) \n",
    "            #compute partition function:\n",
    "            exponential_beta_matrix = np.exp(-beta * element_cluster_distance_matrix)\n",
    "            normalization_partition_funct = (np.multiply(evidence_vector, exponential_beta_matrix)).sum(axis=1)\n",
    "\n",
    "            #update conditionnal cluster_proba\n",
    "            conditional_prob = (np.multiply\n",
    "                                (evidence_vector, \n",
    "                                 exponential_beta_matrix)/ normalization_partition_funct[:,None]\n",
    "                               ).transpose()\n",
    "\n",
    "            #Compute averaged element to cluster distance:\n",
    "            joint_probability = np.multiply(evidence_vector, posteriori_prob.transpose())\n",
    "            d =np.sum(np.multiply(joint_probability,element_cluster_distance_matrix))\n",
    "\n",
    "            D.append(d)\n",
    "            ##Compute Compression rate:       \n",
    "            ratios = posteriori_prob.transpose()/priors\n",
    "            compression_rate_ = np.sum(np.multiply(joint_probability, np.log2(ratios)))\n",
    "            #compression_rate.append(compression_rate_)\n",
    "\n",
    "            ##Compute Lagrange Multiplier:\n",
    "            lagrange_multiplier_ = compression_rate_ + beta*d\n",
    "            #lagrange_multiplier.append(lagrange_multiplier_)     \n",
    "\n",
    "            if len(lagrange_multiplier) > 1:\n",
    "                diff_elem_value = lagrange_multiplier[-2]-lagrange_multiplier[-1]\n",
    "                #diff_lm.append(diff_elem_value)\n",
    "        D_results.append(D[-1])\n",
    "        mean_D_results = sum(D_results)/float(rounds)\n",
    "    return mean_D_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-4fe0a4a092e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcoordenates_beta_range_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbulk_perform_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_matrix_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-81-a0ac873bc1a8>\u001b[0m in \u001b[0;36mbulk_perform_algorithm\u001b[0;34m(range_betas_, prior_, distance_matrix_, range_clusters)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange_betas_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_beta\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblahut_algorithm_imp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdistance_matrix_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mbetas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_beta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-026a84bfa4ed>\u001b[0m in \u001b[0;36mblahut_algorithm_imp\u001b[0;34m(clusters_number, priors, distance_matrix, beta, rounds, threshold)\u001b[0m\n\u001b[1;32m     33\u001b[0m                     element_cluster_distance_matrix[i][c] = sum(\n\u001b[1;32m     34\u001b[0m                                             [posteriori_prob[c][j_idx]* distance_data_point\n\u001b[0;32m---> 35\u001b[0;31m                                              for j_idx, distance_data_point in enumerate(distance_matrix[i])]\n\u001b[0m\u001b[1;32m     36\u001b[0m                                                     ) \n\u001b[1;32m     37\u001b[0m             \u001b[0;31m#compute partition function:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-026a84bfa4ed>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNpts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     element_cluster_distance_matrix[i][c] = sum(\n\u001b[0;32m---> 34\u001b[0;31m                                             [posteriori_prob[c][j_idx]* distance_data_point\n\u001b[0m\u001b[1;32m     35\u001b[0m                                              for j_idx, distance_data_point in enumerate(distance_matrix[i])]\n\u001b[1;32m     36\u001b[0m                                                     ) \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "coordenates_beta_range_10 = bulk_perform_algorithm(range(1,11), prior, distance_matrix_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_betas = range(1,60,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "for i in range_betas:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mine - M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_conditionnal_proba(clusters_number,\n",
    "                              priors,\n",
    "                              distance_matrix):\n",
    "\n",
    "    initial_conditional_prob = initial_clustering_membership_probability(clusters_number)\n",
    "    priors_matrix = np.full((Npts, 1), 1/Npts)\n",
    "\n",
    "    evidence_vector = np.zeros((clusters_number,))\n",
    "    posteriori_prob = np.zeros((Npts, clusters_number)).transpose()\n",
    "    element_cluster_distance_matrix = np.zeros((Npts, clusters_number))\n",
    "    normalization_partition_funct = np.zeros((Npts, ))\n",
    "\n",
    "    for c in range(clusters_number):\n",
    "\n",
    "    #     Computes the evidence for each cluster 'C': returns a vector   \n",
    "        evidence_vector[c] = np.sum(np.dot(initial_conditional_prob[c], priors))\n",
    "    #     Also works:\n",
    "    #     evidence_vector[c] = np.matmul(initial_conditional_prob[c], priors_matrix)\n",
    "\n",
    "    #     Computes the posterior probability: returns a matrix C x Npts\n",
    "        posteriori_prob[c] = (np.dot(priors, initial_conditional_prob[c]))/evidence_vector[c]    \n",
    "    #     Also works: but without transposing posteriori_prob\n",
    "    #     for i in range(Npts):\n",
    "    #         posteriori_prob[i][c] = (initial_conditional_prob[c][i]*priors)/evidence_vector[c]\n",
    "\n",
    "    #     Compute element to cluster distance:\n",
    "        for i in range(Npts):\n",
    "            element_cluster_distance_matrix[i][c] = sum(\n",
    "                                                        [posteriori_prob[c][i]*j for j in distance_matrix_[i]]\n",
    "                                                        )\n",
    "    #compute partition function:\n",
    "    exponential_beta_matrix = np.exp(-beta * element_cluster_distance_matrix)\n",
    "    normalization_partition_funct =  (np.multiply(evidence_vector, exponential_beta_matrix)).sum(axis=1)\n",
    "\n",
    "    #update conditionnal cluster_proba\n",
    "    new_conditionnal_cluster_proba = np.multiply(evidence_vector, exponential_beta_matrix)/ normalization_partition_funct[:,None]\n",
    "\n",
    "    return new_conditionnal_cluster_proba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# careful below there are spoilers for the iteration of the algorithm (though above is the hardest part ;) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_conditionnal_proba(clusters_number,\n",
    "                              priors,\n",
    "                              distance_matrix,\n",
    "                              beta, \n",
    "                              step_number=20,\n",
    "                              threshold=1e-10\n",
    "                             ):\n",
    "\n",
    "    conditional_prob_matrix = initial_clustering_membership_probability(clusters_number)\n",
    "    priors_matrix = np.full((Npts, 1), 1/Npts)\n",
    "    D = []\n",
    "    compression_rate = []\n",
    "    lagrange_multiplier = [101,100]\n",
    "    lagrange_multiplier_ = 1\n",
    "    evidence_vector = np.zeros((clusters_number,))\n",
    "    posteriori_prob = np.zeros((Npts, clusters_number)).transpose()\n",
    "    element_cluster_distance_matrix = np.zeros((Npts, clusters_number))\n",
    "    normalization_partition_funct = np.zeros((Npts, ))\n",
    "    \n",
    "\n",
    "    for n_step in range(step_number):\n",
    "        while lagrange_multiplier[-2]-lagrange_multiplier_> threshold:\n",
    "            #print(lagrange_multiplier[-2] -lagrange_multiplier_)\n",
    "            for c in range(clusters_number):\n",
    "\n",
    "            #     Computes the evidence for each cluster 'C': returns a vector   \n",
    "                evidence_vector[c] = np.sum(np.dot(conditional_prob_matrix[c], priors))\n",
    "            #     Computes the posterior probability: returns a matrix C x Npts\n",
    "                posteriori_prob[c] = (np.dot(priors, conditional_prob_matrix[c]))/evidence_vector[c]    \n",
    "\n",
    "            #     Compute element to cluster distance:\n",
    "                for i in range(Npts):\n",
    "                    element_cluster_distance_matrix[i][c] = sum(\n",
    "                                                                [posteriori_prob[c][i]*j for j in distance_matrix_[i]]\n",
    "                                                                )\n",
    "            #compute partition function:\n",
    "            exponential_beta_matrix = np.exp(-beta * element_cluster_distance_matrix)\n",
    "            normalization_partition_funct =  (np.multiply(evidence_vector, exponential_beta_matrix)).sum(axis=1)\n",
    "\n",
    "            #update conditionnal cluster_proba\n",
    "            conditional_prob_matrix = (np.multiply(evidence_vector, exponential_beta_matrix)/ normalization_partition_funct[:,None]).transpose()\n",
    "\n",
    "            #Compute averaged element to cluster distance:\n",
    "\n",
    "            d =np.sum(\n",
    "                np.multiply(\n",
    "                    evidence_vector,\n",
    "                    (np.multiply(\n",
    "                        posteriori_prob.transpose(), \n",
    "                        element_cluster_distance_matrix)).sum(axis=0)\n",
    "                )\n",
    "            )\n",
    "            D.append(d)\n",
    "            ##Compute Compression rate:\n",
    "\n",
    "            joint_probability = np.multiply(evidence_vector, posteriori_prob.transpose())\n",
    "            ratios = (conditional_prob_matrix).transpose()/priors\n",
    "            compression_rate_ = np.sum(np.multiply(joint_probability, np.log2(ratios)).sum(axis=1))\n",
    "            compression_rate.append(compression_rate_)\n",
    "\n",
    "            ##Compute Lagrange Multiplier:\n",
    "            lagrange_multiplier_ = compression_rate_ + beta*d\n",
    "            lagrange_multiplier.append(lagrange_multiplier_)        \n",
    "\n",
    "    #lagrange_multiplier=lagrange_multiplier[2:]\n",
    "    return D, compression_rate, lagrange_multiplier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_2 = []\n",
    "for i in (range(0, len(D_2)-1)):\n",
    "    d_2.append(D_2[i+1]-D_2[i])\n",
    "    \n",
    "d_3 = []\n",
    "for i in (range(0, len(D_3)-1)):\n",
    "    d_3.append(D_3[i+1]-D_3[i])\n",
    "\n",
    "d_4 = []\n",
    "for i in (range(0, len(D_4)-1)):\n",
    "    d_4.append(D_4[i+1]-D_4[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_2 = []\n",
    "for i in (range(0, len(compression_rate_2)-1)):\n",
    "    cr_2.append(compression_rate_2[i]-compression_rate_2[i+1])\n",
    "    \n",
    "cr_3 = []\n",
    "for i in (range(0, len(compression_rate_3)-1)):\n",
    "    cr_3.append(compression_rate_3[i]-compression_rate_3[i+1])\n",
    "\n",
    "cr_4 = []\n",
    "for i in (range(0, len(compression_rate_4)-1)):\n",
    "    cr_4.append(compression_rate_4[i]-compression_rate_4[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 2\n",
    "prior = 1/Npts\n",
    "D_2, compression_rate_2, lagrange_multiplier_2 = update_conditionnal_proba(2,prior,distance_matrix_,2)\n",
    "D_3, compression_rate_3, lagrange_multiplier_3 = update_conditionnal_proba(3,prior,distance_matrix_,2)\n",
    "D_4, compression_rate_4, lagrange_multiplier_4 = update_conditionnal_proba(4,prior,distance_matrix_,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now iterating the algorithm to watch for convergence:\n",
    "clusters_number = 2\n",
    "beta = 2\n",
    "priors = 1/Npts\n",
    "priors_matrix = np.full((Npts), 1/Npts)\n",
    "#In the first step we initialize our conditional probabilities randomly.\n",
    "conditional_prob = generate_random_clusters_conditional_probability(clusters_number)\n",
    "step_number = 20  # number of iteration of the algorithm\n",
    "for n_step in range(step_number):\n",
    "    # update of the conditionnal probability of cluster given data points\n",
    "    conditional_prob = update_conditionnal_cluster_proba(conditional_prob, priors_matrix, distance_matrix_)\n",
    "    \n",
    "    # computes the distorsion, compression rate etc..\n",
    "    # .... i don't know how to do that ^^\n",
    "    distorsion_rate, compression_rate, lagrangian_value = compute_relevant_metrics(...)\n",
    "    print(distorsion_rate, compression_rate, lagrangian_value)\n",
    "\n",
    "# there would be other ways of specifying when to stop the iteration, you could set a patience value\n",
    "#, that says for instance \" if in the last five iteration the value has changed by less than 0.1% of the best one\n",
    "# i stop the iteration\", or you can look at the discrete derivative and compare to the iterations before... But a \n",
    "# for loop is fine :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr = np.ones((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2.],\n",
       "       [2., 2.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arr*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.71828183, 2.71828183],\n",
       "       [2.71828183, 2.71828183]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1.,1.]) / np.array([2., 2.])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
